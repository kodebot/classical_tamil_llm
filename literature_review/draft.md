# Classical Tamil LLM - Literature Review

## Papers reviewed

### 1. [TamilTamil-Llama: A New Tamil Language Model Based on Llama 2](https://arxiv.org/abs/2311.05845)

LLaMA enhanced with 16k Tamil tokens for better Tamil text generation.

Tamil translated Alpaca dataset, OpenOrca dataset for instruction fine tuning

LoRA for training

### 2. Cited By [Towards building first persian LLM](https://arxiv.org/abs/2312.15713)

Very Similar to #1 but for Persian language

Two methods - 1. LLaMa 2 trained from scratch, 2. LoRA

### 3. Cited By [sphinx: Sample efficient multilingual instruction fine-tuning through n-shot guided prompting](https://arxiv.org/abs/2407.09879)

Multi-lingual instruction fine-tuning using Mistral-7B

## 4. Cited By [Egalitarian Language Representation in Language Models: It All Begins with Tokenizers](https://arxiv.org/abs/2409.11501)

Introduces new Tokenizer called Grapheme Pair Encoding based on Byte Pair Encoding to tokenize languages like Tamil, Hindi and Sinhala better

## 5. Cited By [Chitranuvad: Adapting Multi-lingual LLMs for Multimodal Translation](https://aclanthology.org/2024.wmt-1.80/)

Focus is on image/text to multi-lingual translation for non-English languages

## 6. Cited By [How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of Target Language Text?](https://arxiv.org/abs/2406.11477)

Focus is on vocabulary expansion for non-English languages in low-resource settings

## 7. Cited By [Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models](https://arxiv.org/abs/2410.16168)

Different pretraining strategy for decoder only LLMs

## 8. Cited By [BongLLaMA: LLaMA for Bangla Language](https://arxiv.org/abs/2410.21200)

Similar to Tamil LLaMA but for Bangla (Bengali) language

## 9. Cited By [Suvach -- Generated Hindi QA benchmark](https://arxiv.org/abs/2404.19254)

Proposes a new benchmark to evaluate Question/Answer style chat in Hindi

## 10. Cited By [WIP = Jailbreak Paradox: The Achilles' Heel of LLMs](https://arxiv.org/abs/2406.12702)

Focus is on jailbreak/jailbreak classification of foundation models

## 11. Cited By [Pretraining Data and Tokenizer for Indic LLM](https://arxiv.org/abs/2407.12481)

Focus is on Pretraining Data and Tokenizer

## 12. Cited By [Fine Tuning LLMs for Low Resource Languages](https://ieeexplore.ieee.org/abstract/document/10660753)

Explores options to fine-tune LLM in low resource settings

## 13. Cited By [Tamil Co-Writer: Towards inclusive use of generative AI for writing support](https://ceur-ws.org/Vol-3667/GenAILA-paper9.pdf)

Writing support tool for Tamil writers




## Notes

literature review

data collection - low resource languages

few shot learning or more data

try to use LLM to see mapping

data generation try different models - do qualitative study


